{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "* * *\n",
    "\n",
    "- Likelihood function\n",
    "\n",
    "    $L(\\theta;x_1,...x_n) = f(x_1,...,x_n;\\theta)$ \n",
    "    \n",
    "- Maximum likelihood estimator\n",
    "\n",
    "    $\\hat{\\theta} = argmax_\\theta L(\\theta;x_1,...x_n) $\n",
    "    \n",
    "    - argmax means a subset of $\\theta$ which attains the function's largest value.\n",
    "    \n",
    "- Conditional probability\n",
    "\n",
    "    $p(y|x) = p (Y=y|X=x)$\n",
    "    \n",
    "    - X and Y are Random Variable.\n",
    "    \n",
    "- Conditional likelihood \n",
    "\n",
    "    $L(\\theta;y|x) = p(y|x) = f(y|x;\\theta)$\n",
    "    \n",
    "### Example\n",
    "* * *\n",
    "\n",
    "Suppose X ~ $b(1,p)$, then p.m.f. is $f(x;p)=p^x(1-p)^{(1-x)}$, $p \\in \\Omega$ where $\\theta$ is the parameter space(all possible value of p)\n",
    "\n",
    "Give a random sample $X_1, X_2,..., X_n$ to find a good estimator $ u(X_1, X_2,..., X_n)$ such that $u(x_1, x_2,...,x_n)$ is a good point estimate of p.\n",
    "\n",
    "- $x_1, x_2,...,x_n$ are the observed values of the random sample.\n",
    "\n",
    "So the joint p.m.f is $P(X_1 = x_1,...,X_n = x_n) = \\prod_{i=1}^n p^x_i(1-p)^{(1-x_i)} = p^{\\sum x_i}(1-p)^{(n-\\sum x_i)} $\n",
    "\n",
    "The joint p.m.f., as a function of p, is called likelihood function\n",
    "\n",
    "\\begin{align}\n",
    "L(p) & = L(p;x_1,x_2,...,x_n) \\cr\n",
    "& = f(x_1;p)f(x_2;p)...f(x_n;p) \\cr \n",
    "& = p^{\\sum x_i}(1-p)^{(n-\\sum x_i)}, 0 \\leq p \\leq 1 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find p that maximizes $L(p)$ :\n",
    "\\begin{align}\n",
    "\\frac {\\partial}{\\partial p} L(p) & = (\\sum x_i)P^{\\sum x_i - 1} (1-p)^{n - \\sum x_i} - \n",
    "(n - \\sum x_i)p^{\\sum x_i}(1-p)^{n-\\sum x_i - 1}\\cr\n",
    "\\end{align}\n",
    "\n",
    "Let it equal to zero and restrict 0 < p < 1 :\n",
    "\\begin{align}\n",
    "p^{\\sum x_i}(1-p)^{n-\\sum x_i}(\\frac {\\sum x_i}{p} - \\frac {n - \\sum x_i}{1-p}) = 0\\cr\n",
    "\\end{align}\n",
    "The above function to be zero when\n",
    "\\begin{align}\n",
    "\\frac {\\sum x_i}{p} - \\frac {n - \\sum x_i}{1 -p} = 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$\\because L''(\\bar x) < 0$\n",
    "$\\therefore \\ L(\\bar x) $ is a maximum. $\\bar X $ is called maximum likelihood estimator : \n",
    "$$\\hat p = \\frac{1}{n} \\sum_{i=1}^nX_i = \\bar X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbiased estimator of $\\theta$\n",
    "* * *\n",
    "\n",
    "If $E[u(X_1,X_2,...,X_n)] = \\theta$, then the statistic $u(X_1, X_2,...,X_n)$ is called an unbiased estimator of $\\theta$ . Otherwise, it's said to be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic function\n",
    "\n",
    "* * *\n",
    "\n",
    "\\begin{align}\n",
    "f(x)={\\frac  {L}{1+{\\mathrm  e}^{{-k(x-x_{0})}}}}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "- e = the natural logarithm base (also known as Euler's number),\n",
    "- x0 = the x-value of the sigmoid's midpoint,\n",
    "- L = the curve's maximum value, and\n",
    "- k = the steepness of the curve.[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "* * *\n",
    "\n",
    "Suppose that Y is a binary (Bernoulli) outcome and that x is a real-valued vector. We can assume that the probability that Y = 1 is a nonlinear function of a linear function of x. \n",
    "\n",
    "Specifically, we assume the conditional model\n",
    "\\begin{align}\n",
    "p(Y = 1|x; \\alpha, \\beta) & = \\sigma(\\alpha + \\sum_{j=i}^{d} \\beta_i x_j) \\cr\n",
    "& = \\frac{1}{1 + e^{-[\\alpha + \\sum_{j=1}^d \\beta_j x_j]}} \\cr\n",
    "\\end{align}\n",
    "is the nonlinear function.\n",
    "\n",
    "To simplify this, In Logistic regression, we make essentially the same assumption that\n",
    "\n",
    "\\begin{align}\n",
    "& log \\frac{P(Y=1|x)}{P(Y=0|x)} = w^T x + w_0 \\cr\n",
    "& \\Rightarrow \\frac{p}{1-p} = e^{w^T x + w_0} \\cr\n",
    "& \\Rightarrow {P(Y=1|x)} = \\frac{e^{-w^T x}}{ 1+ e^{-w^T x}}\n",
    "\\end{align}\n",
    "\n",
    "The above two equations have same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower control limit\n",
    "* * *\n",
    "p.548"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* * *\n",
    "- [MLE, Logistic Regression and Stochastic Gradient](http://cseweb.ucsd.edu/~elkan/250B/logreg.pdf)\n",
    "- [Logistic Regression: More details](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
