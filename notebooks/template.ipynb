{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%precision 4\n",
    "%reload_ext autoreload\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrd_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrd_tool import feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import json\n",
    "import findspark\n",
    "import os,sys\n",
    "\n",
    "\n",
    "SCP_ES_HOST = '192.168.10.150'\n",
    "SCP_ES_PORT = 9200\n",
    "SCP_DNS_NETFLOW_INDEX = '*-chewbacca-session*'\n",
    "SCROLL = \"12h\"\n",
    "SIZE = 1000\n",
    "\n",
    "SPARK_JARS = '/root/spark_folder/elasticsearch-hadoop-7.3.1/dist/elasticsearch-spark-20_2.11-7.3.1.jar'\n",
    "SCP_SPARK_MASTER_URL = \"local[8]\"\n",
    "APP_NAME = \"chewbacca session flow\"\n",
    "os.environ['SPARK_HOME'] = \"/root/spark\"\n",
    "sys.path.append(\"/root/spark/python/\")\n",
    "sys.path.append(\"/root/spark/python/lib/py4j-0.10.7-src.zip\")\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "sc_conf = SparkConf() \\\n",
    "            .setMaster(SCP_SPARK_MASTER_URL) \\\n",
    "            .setAppName(APP_NAME) \\\n",
    "            .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "            .set(\"spark.jars\", SPARK_JARS)\n",
    "sc = SparkContext(conf=sc_conf, pyFiles=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNS Log Query\n",
    "query = {\n",
    "    # define required field \n",
    "    \"_source\": [\"dns.status-term\", \"dns.status-term-cnt\", 'timestamp', 'dnsho'],\n",
    "    # filter condition\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\"term\": {\"ipSrc\": \"192.168.0.247\"}},\n",
    "                {\"term\": {\"ipSrc\": \"192.168.10.100\"}}\n",
    "            ],\n",
    "            \"minimum_should_match\" : 1,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "conf = {\"es.nodes\" : SCP_ES_HOST, \n",
    "        \"es.port\" : str(SCP_ES_PORT), \n",
    "        \"es.nodes.data.only\" : \"true\", \n",
    "        \"es.resource.read\" : SCP_DNS_NETFLOW_INDEX,\n",
    "        \"es.query\" : json.dumps(query)\n",
    "       }\n",
    "\n",
    "# create rdd\n",
    "rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,2,3], [4,5,6]], columns=['a', 'b', 'c'])\n",
    "df['cat'] = pd.Categorical(['cat', 'dog'])\n",
    "feature_engineering.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from packaging import version\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), _ = keras.datasets.fashion_mnist.load_data()\n",
    "train_images = train_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log folder\n",
    "logs_base_dir = os.path.abspath(\"./logs/\")\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "logs_fitbase_dir = os.path.join(logs_base_dir, \"fit\")\n",
    "os.makedirs(logs_fitbase_dir, exist_ok=True)\n",
    "logdir = os.path.join(logs_fitbase_dir, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels, \n",
    "    batch_size=64,\n",
    "    epochs=5, \n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_base_dir = \"./logs/\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
